{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf723ed9-d8e0-4f1a-911f-887b927f8569",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#切片\n",
    "from typing import List\n",
    "\n",
    "def split_into_chunks(doc_file: str) -> List[str]:\n",
    "    with open(doc_file, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    return [chunk for chunk in content.split(\"\\n\\n\")]\n",
    "\n",
    "chunks = split_into_chunks(\"doc.md\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"[{i}] {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe9bf60-5d21-4696-99a5-7e7f3b94dd06",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 导入sentence_transformers库\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\n",
    "# 定义嵌入函数，将文本块转换为向量列表\n",
    "def embed_chunk(chunk: str) -> list[float]:\n",
    "    #embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    embedding = embedding_model.encode(chunk, normalize_embeddings=True)\n",
    "    return embedding.tolist()\n",
    "# 测试嵌入函数\n",
    "test_embedding = embed_chunk(\"测试内容\")\n",
    "print(len(test_embedding))\n",
    "print(test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f48192-d9f7-4270-ae08-e5e0300bbb32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 索引化，将文本块转换为向量列表\n",
    "embeddings = [embed_chunk(chunk) for chunk in chunks]\n",
    "\n",
    "print(len(embeddings))\n",
    "vec = embeddings[0]\n",
    "for i in range(0, len(vec), 10):\n",
    "    print(vec[i:i+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babfbd91-76fc-4467-9ff7-ccaf5ffbbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入向量数据库，用于存储和检索向量数据\n",
    "import chromadb\n",
    "\n",
    "chromadb_client = chromadb.EphemeralClient()\n",
    "chromadb_collection = chromadb_client.get_or_create_collection(name=\"default\")\n",
    "\n",
    "def save_embeddings(chunks: List[str], embeddings: List[List[float]]) -> None:\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "        chromadb_collection.add(\n",
    "            documents=[chunk],\n",
    "            embeddings=[embedding],\n",
    "            ids=[str(i)]\n",
    "        )\n",
    "\n",
    "save_embeddings(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47b06d-3f7a-40bd-886a-aca6c7e19f0b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 召回\n",
    "def retrieve(query: str, top_k: int) -> List[str]:\n",
    "    query_embedding = embed_chunk(query)\n",
    "    results = chromadb_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results['documents'][0]\n",
    "\n",
    "query = \"哆啦A梦复制斗篷有什么用？\"\n",
    "retrieved_chunks = retrieve(query, 5)\n",
    "\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"[{i}] {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ac85d-d634-4c1d-93fa-e627cf09a6f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 重排\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank(query: str, retrieved_chunks: List[str], top_k: int) -> List[str]:\n",
    "    cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')\n",
    "    pairs = [(query, chunk) for chunk in retrieved_chunks]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    scored_chunks = list(zip(retrieved_chunks, scores))\n",
    "    scored_chunks.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [chunk for chunk, _ in scored_chunks][:top_k]\n",
    "\n",
    "reranked_chunks = rerank(query, retrieved_chunks, 3)\n",
    "\n",
    "for i, chunk in enumerate(reranked_chunks):\n",
    "    print(f\"[{i}] {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e0823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用DeepSeek的API进行RAG\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "def validate_api_key():\n",
    "    \"\"\"验证 API 密钥是否已设置\"\"\"\n",
    "    if not DEEPSEEK_API_KEY:\n",
    "        raise ValueError(\n",
    "            \"DeepSeek API 密钥未设置！请确保：\\n\"\n",
    "            \"1. 在项目根目录创建 .env 文件\\n\"\n",
    "            \"2. 在 .env 文件中添加：DEEPSEEK_API_KEY=你的API密钥\\n\"\n",
    "            \"3. 确保 API 密钥格式正确且未过期\"\n",
    "        )\n",
    "\n",
    "# 加载环境变量并验证\n",
    "load_dotenv()\n",
    "DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')\n",
    "DEEPSEEK_API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "\n",
    "def generate(query: str, chunks: List[str]) -> str:\n",
    "    try:\n",
    "        # 验证 API 密钥\n",
    "        validate_api_key()\n",
    "        \n",
    "        # 构建提示词\n",
    "        prompt = f\"\"\"你是一位知识助手，请根据用户的问题和下列片段生成准确的回答。\n",
    "\n",
    "用户问题: {query}\n",
    "\n",
    "相关片段:\n",
    "{\"\".join(chunks)}\n",
    "请基于上述内容作答，不要编造信息。\"\"\"\n",
    "\n",
    "        print(\"正在生成回答...\\n\")\n",
    "        print(f\"提示词内容:\\n{prompt}\\n\")\n",
    "\n",
    "        # 设置请求头\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\"\n",
    "        }\n",
    "\n",
    "        # 构建请求体\n",
    "        data = {\n",
    "            \"model\": \"deepseek-chat\",  # 使用正确的模型名称\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "\n",
    "        # 发送 POST 请求\n",
    "        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data)\n",
    "        \n",
    "        # 如果是认证错误，给出更详细的错误信息\n",
    "        if response.status_code == 401:\n",
    "            print(\"认证错误！请检查：\")\n",
    "            print(\"1. API 密钥是否正确设置在 .env 文件中\")\n",
    "            print(\"2. API 密钥是否有效（未过期）\")\n",
    "            print(\"3. API 密钥格式是否正确\")\n",
    "            print(f\"错误详情: {response.text}\")\n",
    "            return \"\"\n",
    "            \n",
    "        # 检查其他错误\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # 解析 JSON 响应\n",
    "        result = response.json()\n",
    "        return result['choices'][0]['message']['content']\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"API 请求错误: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"错误详情: {e.response.text}\")\n",
    "        return \"\"\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"解析响应出错: {e}\")\n",
    "        return \"\"\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return \"\"\n",
    "answer = generate(query, reranked_chunks)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c020279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import google.generativeai as genai\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# # 从环境变量中获取 API 密钥\n",
    "# api_key = os.getenv('GEMINI_API_KEY')\n",
    "# # 配置 API 密钥\n",
    "# genai.configure(api_key=api_key)\n",
    "\n",
    "# def generate(query: str, chunks: list[str]) -> str:\n",
    "#     prompt = f\"\"\"你是一位知识助手，请根据用户的问题和下列片段生成准确的回答。\n",
    "# 用户问题: {query}\n",
    "# 相关片段:\n",
    "# {\"\".join(chunks)}\n",
    "# 请基于上述内容作答，不要编造信息。\"\"\"\n",
    "#     print(f\"{prompt}\\n\\n---\\n\")\n",
    "#     # 创建模型实例\n",
    "#     model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "#     # 生成内容\n",
    "#     response = model.generate_content(prompt)\n",
    "#     # 返回响应文本，如果没有则返回空字符串\n",
    "#     return response.text if response.text else \"\"\n",
    "\n",
    "# answer = generate(query, reranked_chunks)\n",
    "# print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
