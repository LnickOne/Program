{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件下载成功！\n",
      "文件读取成功\n"
     ]
    }
   ],
   "source": [
    "# 下载数据集\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 定义数据集的 URL 和校验信息\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'  # 假设这是 d2l.DATA_URL 的值\n",
    "DATA_HUB = {}\n",
    "DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt', '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def download_time_machine():\n",
    "    # 使用定义的 URL 来获取文件内容\n",
    "    url = DATA_HUB['time_machine'][0]\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        content = response.text\n",
    "        data_dir = './DataSet'\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)\n",
    "        file_path = os.path.join(data_dir, 'timemachine.txt')\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print('文件下载成功！')\n",
    "    else:\n",
    "        print('文件下载失败！')\n",
    "download_time_machine() \n",
    "\n",
    "\n",
    "# 验证文件\n",
    "try:\n",
    "    with open('./DataSet/timemachine.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    print(\"文件读取成功\")\n",
    "except FileNotFoundError:\n",
    "    print(\"文件未找到\")\n",
    "except UnicodeDecodeError:\n",
    "    print(\"文件编码错误\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "import collections\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "def read_time_machine():\n",
    "    \"\"\"将《时间机器》数据集加载到文本行列表中。\"\"\"\n",
    "    with open(\"./DataSet/timemachine.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub(\"[^A-Za-z]+\", \" \", line).strip().lower() for line in lines]\n",
    "\n",
    "\n",
    "lines = read_time_machine()\n",
    "print(f\"# 文本总行数: {len(lines)}\")\n",
    "print(lines[0])\n",
    "print(lines[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集分词\n",
    "# 下面的`tokenize`函数将文本行列表（`lines`）作为输入，\n",
    "# 列表中的每个元素是一个文本序列（如一条文本行）。\n",
    "# [**每个文本序列又被拆分成一个词元列表**]，*词元*（token）是文本的基本单位。\n",
    "# 最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。\n",
    "\n",
    "\n",
    "def tokenize(lines, token=\"word\"):  # @save\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == \"word\":\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == \"char\":\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print(\"错误：未知词元类型：\" + token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([32, 49])\n",
      "目标形状: torch.Size([32, 49])\n"
     ]
    }
   ],
   "source": [
    "# 将数据集构建词汇表\n",
    "# 词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。\n",
    "# 现在，让我们[**构建一个字典，通常也叫做*词表*（vocabulary），\n",
    "# 用来将字符串类型的词元映射到从$0$开始的数字索引中**]。\n",
    "# 我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，\n",
    "# 得到的统计结果称之为*语料*（corpus）。\n",
    "# 然后根据每个唯一词元的出现频率，为其分配一个数字索引。\n",
    "# 很少出现的词元通常被移除，这可以降低复杂性。\n",
    "# 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“&lt;unk&gt;”。\n",
    "# 我们可以选择增加一个列表，用于保存那些被保留的词元，\n",
    "# 例如：填充词元（“&lt;pad&gt;”）；\n",
    "# 序列开始词元（“&lt;bos&gt;”）；\n",
    "# 序列结束词元（“&lt;eos&gt;”）。\n",
    "\n",
    "# 使用的设备\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "class Vocab:  # @save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(\n",
    "            counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx for idx,\n",
    "                             token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "    @property\n",
    "    def itos(self):\n",
    "        \"\"\"从索引到词元的映射列表\"\"\"\n",
    "        return self.idx_to_token\n",
    "\n",
    "\n",
    "def count_corpus(tokens):  # @save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "\n",
    "# 加载语料库\n",
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "# 填充与截断\n",
    "def pad_truncate(sequence, max_length, padding_index=0):\n",
    "    \"\"\"对序列进行填充或截断操作。\"\"\"\n",
    "    if len(sequence) < max_length:\n",
    "        return sequence + [padding_index] * (max_length - len(sequence))\n",
    "    else:\n",
    "        return sequence[:max_length]\n",
    "\n",
    "\n",
    "# 创建数据集\n",
    "class TimeMachineDataset(Dataset):\n",
    "    def __init__(self, corpus, max_length):\n",
    "        self.corpus = [pad_truncate(seq, max_length) for seq in corpus]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.corpus[idx]\n",
    "        input_seq = torch.tensor(sequence[:-1])\n",
    "        target_seq = torch.tensor(sequence[1:])\n",
    "        return input_seq, target_seq\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "# 假设将数据分割成长度为 50 的序列\n",
    "max_length = 50\n",
    "sequences = [corpus[i:i+max_length]\n",
    "             for i in range(0, len(corpus), max_length) if len(corpus[i:i+max_length]) == max_length]\n",
    "\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = TimeMachineDataset(sequences, max_length)\n",
    "# 设置 drop_last=True 丢弃最后一个不完整的批次\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "# 使用的设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 打印数据加载器中的一个批次\n",
    "for inputs, targets in dataloader:\n",
    "    # 将输入和目标数据移动到 CUDA 设备\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    print(f'输入形状: {inputs.shape}')\n",
    "    print(f'目标形状: {targets.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "class RNN_Model(Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, device):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.device = device\n",
    "        self.params = self.get_params()\n",
    "\n",
    "    def get_params(self):\n",
    "        num_inputs = num_outputs = self.vocab_size\n",
    "        def normal(shape):\n",
    "            return torch.randn(size=shape, device=self.device) * 0.01\n",
    "        # 输入到隐藏层的权重\n",
    "        W_xh = normal((num_inputs, self.num_hiddens))\n",
    "        # 隐藏层到隐藏层的权重\n",
    "        W_hh = normal((self.num_hiddens, self.num_hiddens))\n",
    "        # 隐藏层的偏置\n",
    "        b_h = torch.zeros(self.num_hiddens, device=self.device)\n",
    "        # 隐藏层到输出层的权重\n",
    "        W_hq = normal((self.num_hiddens, num_outputs))\n",
    "        # 输出层的偏置\n",
    "        b_q = torch.zeros(num_outputs, device=self.device)\n",
    "        params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "        for param in params:\n",
    "            param.requires_grad_(True)\n",
    "        return params\n",
    "    def init_rnn_state(self, batch_size):\n",
    "        return (torch.zeros((batch_size, self.num_hiddens), device=self.device),)\n",
    "\n",
    "\n",
    "    def rnn_forward(self, X, state):\n",
    "        #print(f\"State type: {type(state)}, State shape: {state[0].shape if isinstance(state, tuple) else state.shape}\")\n",
    "        # 从 params 中获取权重和偏置\n",
    "        W_xh, W_hh, b_h, W_hq, b_q = self.params\n",
    "        # 初始化 outputs 列表\n",
    "        outputs = []\n",
    "        # 初始化隐藏状态 H\n",
    "        H, = state\n",
    "        # 交换 X 的维度，使得形状为 (seq_length, batch_size)\n",
    "        X = X.transpose(0, 1)\n",
    "        for x in X:\n",
    "            x = torch.nn.functional.one_hot(x, num_classes=self.vocab_size).float()\n",
    "            # 确保 x 的数据类型和 W_xh 一致\n",
    "            x = x.to(W_xh.dtype)\n",
    "            #print(f\"x shape: {x.shape}\")\n",
    "            # 确保 H 的数据类型和 W_hh 一致\n",
    "            H = H.to(W_hh.dtype)\n",
    "            H = torch.tanh(torch.mm(x, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "            Y = torch.mm(H, W_hq) + b_q\n",
    "            outputs.append(Y)\n",
    "        output = torch.stack(outputs, dim=0).transpose(0, 1).contiguous().view(-1, self.vocab_size)\n",
    "        #print(f\"Output shape after rnn_forward: {output.shape}\")\n",
    "        return output, (H,)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X, state):  # 修改 forward 方法以接受 state 参数\n",
    "        output, new_state = self.rnn_forward(X, state)\n",
    "        return output, new_state\n",
    "\n",
    "\n",
    "def grad_clipping(model, theta):  # @save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(model, torch.nn.Module):\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = model.params\n",
    "    # 筛选出有梯度的参数\n",
    "    grads = [p.grad for p in params if p.grad is not None]\n",
    "    if len(grads) == 0:  # 如果没有梯度，直接返回\n",
    "        return  \n",
    "    # 使用 torch.stack 和 torch.sum 来计算梯度的平方和\n",
    "    grad_squared_sum = torch.sum(torch.stack(\n",
    "        [torch.sum(g ** 2) for g in grads]))\n",
    "    norm = torch.sqrt(grad_squared_sum)\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            if param.grad is not None:\n",
    "                param.grad[:] *= theta / norm\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_iter, lr, num_epochs, theta, device):\n",
    "    optimizer = optim.SGD(model.params, lr=lr)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        state = None\n",
    "        metric = [0.0] * 2  # 训练损失之和, 词元数量\n",
    "        for X, Y in train_iter:\n",
    "            if state is None:\n",
    "                state = model.init_rnn_state(X.shape[0])\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            output, state = model(X, state)\n",
    "            # print(f\"Output shape before reshape: {output.shape}\")\n",
    "            # print(f\"Target shape before reshape: {Y.shape}\")\n",
    "            y = Y.view(-1)  # 调整目标标签形状\n",
    "            # print(f\"Output shape after reshape: {output.shape}\")\n",
    "            # print(f\"Target shape after reshape: {y.shape}\")\n",
    "            if output.size(0) != y.size(0):\n",
    "                raise ValueError(\n",
    "                    f\"Output batch size ({output.size(0)}) does not match target batch size ({y.size(0)}).\")\n",
    "            l = loss(output, y.long()).mean()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(model, theta)\n",
    "            optimizer.step()\n",
    "            metric[0] += l * y.numel()\n",
    "            metric[1] += y.numel()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'epoch {epoch + 1}/{num_epochs}, loss {metric[0] / metric[1]:.3f}')\n",
    "\n",
    "def predict(model, prefix, num_preds, device):\n",
    "    state = model.init_rnn_state(1)\n",
    "    outputs = [vocab[prefix[0]]]  # 使用词汇表将字符转换为索引\n",
    "\n",
    "    def get_input():\n",
    "        return torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = model(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测 num_preds 步\n",
    "        y, state = model(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.itos[index] for index in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10/200, loss 2.081\n",
      "epoch 20/200, loss 1.891\n",
      "epoch 30/200, loss 1.759\n",
      "epoch 40/200, loss 1.669\n",
      "epoch 50/200, loss 1.607\n",
      "epoch 60/200, loss 1.560\n",
      "epoch 70/200, loss 1.528\n",
      "epoch 80/200, loss 1.503\n",
      "epoch 90/200, loss 1.486\n",
      "epoch 100/200, loss 1.468\n",
      "epoch 110/200, loss 1.458\n",
      "epoch 120/200, loss 1.451\n",
      "epoch 130/200, loss 1.441\n",
      "epoch 140/200, loss 1.431\n",
      "epoch 150/200, loss 1.428\n",
      "epoch 160/200, loss 1.420\n",
      "epoch 170/200, loss 1.417\n",
      "epoch 180/200, loss 1.410\n",
      "epoch 190/200, loss 1.406\n",
      "epoch 200/200, loss 1.406\n"
     ]
    }
   ],
   "source": [
    "# 1. 定义必要的参数\n",
    "vocab_size = len(vocab)  # 使用前面加载语料库时生成的词汇表大小\n",
    "num_hiddens = 256\n",
    "lr = 1\n",
    "num_epochs = 200\n",
    "theta = 0.5\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# 2. 实例化模型\n",
    "model = RNN_Model(vocab_size, num_hiddens, device)\n",
    "\n",
    "# 3. 调用训练函数\n",
    "train(model, dataloader, lr, num_epochs, theta, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果: time traveller s soon and the same the same s see at first the same s suggestion of the struck and conster and exc\n"
     ]
    }
   ],
   "source": [
    "# 4. 调用预测函数\n",
    "prefix = \"time traveller\"  # 预测的前缀\n",
    "num_preds = 100  # 预测的字符数量\n",
    "result = predict(model, prefix, num_preds, device)\n",
    "print(\"预测结果:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
