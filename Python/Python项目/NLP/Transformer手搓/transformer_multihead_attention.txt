不过实际中，q、k、v会先经过线性变换，从原始的512维降维到64维。
比如对于长度为3的输入序列，每个头处理后的q、k、v矩阵维度都是3×64。
接着，对每组q、k、v执行注意力公式计算，得到3×64的注意力输出结果。

所谓「多头注意力」，就是这样的线性变换和注意力计算会并行执行8次（也就是8个头），
因此会得到8组3×64的输出矩阵。然后把这8组矩阵沿着特征维度拼接起来，
就得到一个3×512的矩阵（因为8×64=512）。
最后，这个拼接后的矩阵会经过残差连接和层归一化处理，输出维度仍然保持3×512不变。