{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "训练集的 batch 数量: 196，每个 batch 中的数据数量: 256\n",
      "测试集的 batch 数量: 40，每个 batch 中的数据数量: 256\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "# 下载数据集的路径\n",
    "path = './DataSet'\n",
    "# 使用的设备\n",
    "device = torch.device(\"cuda\")\n",
    "# 数据预处理,转换为张量并归一化\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将图像大小调整为 224x224\n",
    "    transforms.ToTensor(),  # 将图像转换为张量\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # 归一化处理\n",
    "\n",
    "train_set = CIFAR10(root=path, train=True, download=True, transform=transform)\n",
    "test_set = CIFAR10(root=path, train=False, download=True, transform=transform)\n",
    "# 加载数据集并设置批次大小\n",
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "# 打印数据集的信息\n",
    "print(\n",
    "    f\"训练集的 batch 数量: {len(train_loader)}，每个 batch 中的数据数量: {train_loader.batch_size}\")\n",
    "print(\n",
    "    f\"测试集的 batch 数量: {len(test_loader)}，每个 batch 中的数据数量: {test_loader.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Flatten\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import AdaptiveAvgPool2d\n",
    "import torch\n",
    "\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "\n",
    "class GoogLeNet(Module):\n",
    "    class Inception(Module):\n",
    "        # c1--c4是每条路径的输出通道数\n",
    "        def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "            super(GoogLeNet.Inception, self).__init__(**kwargs)\n",
    "            # 线路1，单1x1卷积层\n",
    "            self.p1 = Sequential(\n",
    "                Conv2d(in_channels, c1, kernel_size=1),\n",
    "                ReLU()\n",
    "            )\n",
    "            kaiming_uniform_(self.p1[0].weight, nonlinearity='relu')\n",
    "            # 线路2，1x1卷积层后接3x3卷积层\n",
    "            self.p2 = Sequential(\n",
    "                Conv2d(in_channels, c2[0], kernel_size=1),\n",
    "                ReLU(),\n",
    "                Conv2d(c2[0], c2[1], kernel_size=3, padding=1),\n",
    "                ReLU()\n",
    "            )\n",
    "            kaiming_uniform_(self.p2[0].weight, nonlinearity='relu')\n",
    "            kaiming_uniform_(self.p2[2].weight, nonlinearity='relu')\n",
    "            # 线路3，1x1卷积层后接5x5卷积层\n",
    "            self.p3 = Sequential(\n",
    "                Conv2d(in_channels, c3[0], kernel_size=1),\n",
    "                ReLU(),\n",
    "                Conv2d(c3[0], c3[1], kernel_size=5, padding=2),\n",
    "                ReLU()\n",
    "            )\n",
    "            kaiming_uniform_(self.p3[0].weight, nonlinearity='relu')\n",
    "            kaiming_uniform_(self.p3[2].weight, nonlinearity='relu')\n",
    "            # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "            self.p4 = Sequential(\n",
    "                MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                Conv2d(in_channels, c4, kernel_size=1),\n",
    "                ReLU()\n",
    "            )\n",
    "            kaiming_uniform_(self.p4[1].weight, nonlinearity='relu')\n",
    "\n",
    "        def forward(self, x):\n",
    "            p1 = self.p1(x)\n",
    "            p2 = self.p2(x)\n",
    "            p3 = self.p3(x)\n",
    "            p4 = self.p4(x)\n",
    "            # 在通道维度上连结输出\n",
    "            return torch.cat((p1, p2, p3, p4), dim=1)\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = Sequential(\n",
    "            Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        kaiming_uniform_(self.block1[0].weight, nonlinearity='relu')\n",
    "        self.block2 = Sequential(\n",
    "            Conv2d(64, 64, kernel_size=1),\n",
    "            ReLU(),\n",
    "            Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        kaiming_uniform_(self.block2[0].weight, nonlinearity='relu')\n",
    "        kaiming_uniform_(self.block2[2].weight, nonlinearity='relu')\n",
    "        self.block3 = Sequential(\n",
    "            GoogLeNet.Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "            GoogLeNet.Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.block4 = Sequential(\n",
    "            GoogLeNet.Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "            GoogLeNet.Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "            GoogLeNet.Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "            GoogLeNet.Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "            GoogLeNet.Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.block5 = Sequential(\n",
    "            GoogLeNet.Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "            GoogLeNet.Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "            AdaptiveAvgPool2d((1, 1)),\n",
    "            Flatten(),\n",
    "            Linear(1024, 10)\n",
    "        )\n",
    "        kaiming_uniform_(self.block5[4].weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "# 定义损失函数和优化器\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "import time\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "def train_model(train_loader, model):\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # 训练的轮数\n",
    "    num_epochs = 10\n",
    "    # 初始化 running_loss\n",
    "    running_loss = 0.0\n",
    "    # 记录训练开始时间\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 混合精度训练的梯度缩放器\n",
    "    scaler = GradScaler()\n",
    "    # 梯度累积步数\n",
    "    accumulation_steps = 1\n",
    "\n",
    "    # 遍历 epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # 遍历训练数据\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # 将数据移动到 GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast('cuda'):\n",
    "                # 计算模型输出\n",
    "                y_hat = model(inputs)\n",
    "                # 计算损失\n",
    "                loss = criterion(y_hat, labels)\n",
    "                # 梯度累积\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            # 反向传播\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # 梯度累积更新\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                # 更新参数\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # 梯度清零\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            # 释放中间变量和清空缓存\n",
    "            del inputs, labels, y_hat, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # 打印每个 epoch 的损失\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')\n",
    "        # 每个 epoch 结束后重置 running_loss\n",
    "        running_loss = 0.0\n",
    "    # 记录训练结束时间\n",
    "    end_time = time.time()\n",
    "    # 计算训练总时间\n",
    "    total_time = end_time - start_time\n",
    "    # 转换为分钟和秒\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = total_time % 60\n",
    "    print(\n",
    "        f'Training completed in {minutes} minutes and {seconds:.2f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "\n",
    "\n",
    "def evaluate_model(test_loader, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        # 将数据移动到GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 计算模型输出\n",
    "        y_hat = model(inputs)\n",
    "        # 转换为 numpy 数据类型\n",
    "        y_hat = y_hat.detach().cpu().numpy()\n",
    "        actual = labels.cpu().numpy()\n",
    "        # 转换为类标签\n",
    "        y_hat = argmax(y_hat, axis=1)\n",
    "        # 为stack格式化\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        y_hat = y_hat.reshape((len(y_hat), 1))\n",
    "        # 存储\n",
    "        predictions.append(y_hat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # 计算准确率\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.92929972921099\n",
      "Epoch 2/10, Loss: 1.5499276293783772\n",
      "Epoch 3/10, Loss: 1.3434604345535746\n",
      "Epoch 4/10, Loss: 1.2010833140538664\n",
      "Epoch 5/10, Loss: 1.0634814902227752\n",
      "Epoch 6/10, Loss: 0.9486277039561953\n",
      "Epoch 7/10, Loss: 0.8491951874932464\n",
      "Epoch 8/10, Loss: 0.776390801887123\n",
      "Epoch 9/10, Loss: 0.6944063094501592\n",
      "Epoch 10/10, Loss: 0.6386481030863158\n",
      "Training completed in 12 minutes and 20.51 seconds.\n",
      "Overall Accuracy: 0.690\n"
     ]
    }
   ],
   "source": [
    "# 产生实例,并且将实例放入GPU\n",
    "GoogLeNet_model = GoogLeNet().to(device)\n",
    "\n",
    "# 训练实例\n",
    "train_model(train_loader, GoogLeNet_model)\n",
    "# 评估实例整体准确率\n",
    "acc = evaluate_model(test_loader, GoogLeNet_model)\n",
    "print('Overall Accuracy: %.3f' % acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
