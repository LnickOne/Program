{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "训练集的 batch 数量: 98，每个 batch 中的数据数量: 512\n",
      "测试集的 batch 数量: 20，每个 batch 中的数据数量: 512\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "# 下载数据集的路径\n",
    "path = './DataSet'\n",
    "# 使用的设备\n",
    "device = torch.device(\"cuda\")\n",
    "# 数据预处理,转换为张量并归一化\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将图像大小调整为 224x224\n",
    "    transforms.ToTensor(),  # 将图像转换为张量\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # 归一化处理\n",
    "\n",
    "train_set = CIFAR10(root=path, train=True, download=True, transform=transform)\n",
    "test_set = CIFAR10(root=path, train=False, download=True, transform=transform)\n",
    "# 加载数据集并设置批次大小\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "# 修正 print 语句，明确输出内容\n",
    "print(f\"训练集的 batch 数量: {len(train_loader)}，每个 batch 中的数据数量: {train_loader.batch_size}\")\n",
    "print(f\"测试集的 batch 数量: {len(test_loader)}，每个 batch 中的数据数量: {test_loader.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Flatten\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Sequential\n",
    "\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "\n",
    "class AlexNet(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 定义特征提取部分\n",
    "        self.feature = Sequential(\n",
    "            # 卷积层 1：输入通道数为 3，输出通道数为 96，卷积核大小为 11x11，步幅为 4，填充为 1\n",
    "            # 使用 Kaiming 初始化方法初始化卷积层 1 的权重，适用于 ReLU 激活函数\n",
    "            # 池化层 1：使用 3x3 的最大池化，步长为 2\n",
    "            Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1)),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=(3, 3), stride=(2, 2)),\n",
    "            # 卷积层 2：输入通道数为 96，输出通道数为 256，卷积核大小为 5x5，填充为 2\n",
    "            # 使用 Kaiming 初始化方法初始化卷积层 2 的权重，适用于 ReLU 激活函数\n",
    "            # 池化层 2：使用 3x3 的最大池化，步长为 2\n",
    "            Conv2d(96, 256, kernel_size=(5, 5), padding=(2, 2)),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=(3, 3), stride=(2, 2)),\n",
    "            # 卷积层 3：输入通道数为 256，输出通道数为 384，卷积核大小为 3x3，填充为 1\n",
    "            # 使用 Kaiming 初始化方法初始化卷积层 3 的权重，适用于 ReLU 激活函数\n",
    "            Conv2d(256, 384, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            ReLU(),\n",
    "            # 卷积层 4：输入通道数为 384，输出通道数为 384，卷积核大小为 3x3，填充为 1\n",
    "            # 使用 Kaiming 初始化方法初始化卷积层 4 的权重，适用于 ReLU 激活函数\n",
    "            Conv2d(384, 384, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            ReLU(),\n",
    "            # 卷积层 5：输入通道数为 384，输出通道数为 256，卷积核大小为 3x3，填充为 1\n",
    "            # 使用 Kaiming 初始化方法初始化卷积层 5 的权重，适用于 ReLU 激活函数\n",
    "            Conv2d(384, 256, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            ReLU(),\n",
    "            # 做了三次卷积之后，才进行一次池化\n",
    "            # 池化层 3：使用 3x3 的最大池化，步长为 2\n",
    "            MaxPool2d(kernel_size=(3, 3), stride=(2, 2))\n",
    "        )\n",
    "        # 对特征提取部分的卷积层进行 Kaiming 初始化\n",
    "        for layer in self.feature:\n",
    "            if isinstance(layer, Conv2d):\n",
    "                kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "        # 定义分类部分\n",
    "        self.classifier = Sequential(\n",
    "            # 添加 Flatten 层\n",
    "            Flatten(),\n",
    "            # 全连接层 1：输入维度为 6400，输出维度为 4096\n",
    "            # 使用 Kaiming 初始化方法初始化全连接层 1 的权重，适用于 ReLU 激活函数，并且使用dropout\n",
    "            Linear(6400, 4096),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.5),\n",
    "            # 全连接层 2：输入维度为 4096，输出维度为 4096\n",
    "            # 使用 Kaiming 初始化方法初始化全连接层 2 的权重，适用于 ReLU 激活函数，并且使用dropout\n",
    "            Linear(4096, 4096),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.5),\n",
    "            # 输出层：输入维度为 4096，输出维度为 10，对应 CIFAR-10 的 10 个类别\n",
    "            Linear(4096, 10)\n",
    "        )\n",
    "        # 对分类器部分的全连接层进行 Kaiming 初始化\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, Linear):\n",
    "                if layer is not self.classifier[-1]:\n",
    "                    kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "                else:\n",
    "                    xavier_uniform_(layer.weight)\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "# 定义损失函数和优化器\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "import time\n",
    "\n",
    "def train_model(train_loader, model):\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # 训练的轮数\n",
    "    num_epochs = 10\n",
    "    # 初始化 running_loss\n",
    "    running_loss = 0.0\n",
    "    # 记录训练开始时间\n",
    "    start_time = time.time()\n",
    "    # 遍历epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # 遍历训练数据\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # 将数据移动到GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 计算模型输出\n",
    "            y_hat = model(inputs)\n",
    "            # 计算损失\n",
    "            loss = criterion(y_hat, labels)\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        # 打印每个 epoch 的损失\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')\n",
    "        # 每个 epoch 结束后重置 running_loss\n",
    "        running_loss = 0.0\n",
    "    # 记录训练结束时间\n",
    "    end_time = time.time()\n",
    "    # 计算训练总时间\n",
    "    total_time = end_time - start_time\n",
    "    # 转换为分钟和秒\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = total_time % 60\n",
    "    print(f'Training completed in {minutes} minutes and {seconds:.2f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "\n",
    "\n",
    "def evaluate_model(test_loader, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        # 将数据移动到GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 计算模型输出\n",
    "        y_hat = model(inputs)\n",
    "        # 转换为 numpy 数据类型\n",
    "        y_hat = y_hat.detach().cpu().numpy()\n",
    "        actual = labels.cpu().numpy()\n",
    "        # 转换为类标签\n",
    "        y_hat = argmax(y_hat, axis=1)\n",
    "        # 为stack格式化\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        y_hat = y_hat.reshape((len(y_hat), 1))\n",
    "        # 存储\n",
    "        predictions.append(y_hat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # 计算准确率\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.0321377369822287\n",
      "Epoch 2/10, Loss: 1.470605987675336\n",
      "Epoch 3/10, Loss: 1.2526810315190529\n",
      "Epoch 4/10, Loss: 1.105035921140593\n",
      "Epoch 5/10, Loss: 0.9831318691068766\n",
      "Epoch 6/10, Loss: 0.8522634846823556\n",
      "Epoch 7/10, Loss: 0.7738189052562324\n",
      "Epoch 8/10, Loss: 0.7127545092787061\n",
      "Epoch 9/10, Loss: 0.6578454721947106\n",
      "Epoch 10/10, Loss: 0.590369524700301\n",
      "Training completed in 7 minutes and 28.81 seconds.\n",
      "Overall Accuracy: 0.744\n"
     ]
    }
   ],
   "source": [
    "# 产生实例,并且将实例放入GPU\n",
    "AlexNet_model = AlexNet()\n",
    "AlexNet_model.to(device)\n",
    "# 训练实例\n",
    "train_model(train_loader, AlexNet_model)\n",
    "# 评估实例整体准确率\n",
    "acc = evaluate_model(test_loader, AlexNet_model)\n",
    "print('Overall Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存训练好的模型\n",
    "PATH = '../DataSet/model/AlexNet.pth'\n",
    "torch.save(AlexNet_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for plane: 88.4%\n",
      "Accuracy for car  : 92.8%\n",
      "Accuracy for bird : 62.3%\n",
      "Accuracy for cat  : 58.7%\n",
      "Accuracy for deer : 81.5%\n",
      "Accuracy for dog  : 59.6%\n",
      "Accuracy for frog : 82.2%\n",
      "Accuracy for horse: 73.1%\n",
      "Accuracy for ship : 80.6%\n",
      "Accuracy for truck: 81.3%\n"
     ]
    }
   ],
   "source": [
    "AlexNet_model = AlexNet().to(device)\n",
    "AlexNet_model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# 初始化分类准确率统计\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 评估每个类别的准确率\n",
    "with torch.no_grad():\n",
    "    AlexNet_model.eval()  # 确保模型处于评估模式\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        # 将数据移动到GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = AlexNet_model(images)  # 使用实例化模型\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        # 将预测结果和标签移动到CPU并转换为numpy数组\n",
    "        labels = labels.cpu().numpy()\n",
    "        predictions = predictions.cpu().numpy()\n",
    "\n",
    "        for label_idx, pred_idx in zip(labels, predictions):\n",
    "            class_name = classes[label_idx]\n",
    "            if label_idx == pred_idx:\n",
    "                correct_pred[class_name] += 1\n",
    "            total_pred[class_name] += 1\n",
    "\n",
    "# 打印每个类别的准确率\n",
    "for classname in classes:\n",
    "    total = total_pred[classname]\n",
    "    correct = correct_pred[classname]\n",
    "    if total == 0:\n",
    "        print(f\"Class {classname} has no test samples\")\n",
    "        continue\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy for {classname:5s}: {accuracy:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
