{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "训练集的 batch 数量: 782，每个 batch 中的数据数量: 64\n",
      "测试集的 batch 数量: 157，每个 batch 中的数据数量: 64\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# 下载数据集的路径\n",
    "path = \"./DataSet\"\n",
    "# 使用的设备\n",
    "device = torch.device(\"cuda\")\n",
    "# 数据预处理,转换为张量并归一化\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224,   224)),  # 将图像大小调整为 224x224\n",
    "        transforms.ToTensor(),  # 将图像转换为张量\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")  # 归一化处理\n",
    "\n",
    "train_set = CIFAR10(root=path, train=True, download=True, transform=transform)\n",
    "test_set = CIFAR10(root=path, train=False, download=True, transform=transform)\n",
    "# 加载数据集并设置批次大小\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "# 修正 print 语句，明确输出内容\n",
    "print(\n",
    "    f\"训练集的 batch 数量: {len(train_loader)}，每个 batch 中的数据数量: {train_loader.batch_size}\"\n",
    ")\n",
    "print(\n",
    "    f\"测试集的 batch 数量: {len(test_loader)}，每个 batch 中的数据数量: {test_loader.batch_size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Flatten\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Sequential\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "class VggNet(Module):\n",
    "    def vgg_block(self, num_convs, in_channels, out_channels):\n",
    "        layers = []\n",
    "        for _ in range(num_convs):\n",
    "            layers.append(Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)))\n",
    "            layers.append(ReLU())\n",
    "            in_channels = out_channels\n",
    "        layers.append(MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
    "        return Sequential(*layers)\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # VGGNet11的配置\n",
    "        # conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "        # VGGNet13的配置\n",
    "        # conv_arch = [(2, 64), (2, 128), (2, 256), (2, 512), (2, 512)]\n",
    "        # VGGNet16的配置\n",
    "        # conv_arch = [(2, 64), (2, 128), (3, 256), (3, 512), (3, 512)]\n",
    "        # VGGNet19的配置\n",
    "        conv_arch = [(2, 64), (2, 128), (4, 256), (4, 512), (4, 512)]\n",
    "        in_channels = 3\n",
    "        # 定义特征提取部分\n",
    "        self.features = Sequential()\n",
    "        for i, (num_convs, out_channels) in enumerate(conv_arch):\n",
    "            conv_blk = self.vgg_block(num_convs, in_channels, out_channels)\n",
    "            self.features.add_module(f'conv_block_{i}', conv_blk)\n",
    "            in_channels = out_channels\n",
    "        # 对特征提取部分的卷积层进行 Kaiming 初始化\n",
    "        for layer in self.features:\n",
    "            if isinstance(layer, Conv2d):\n",
    "                kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "        # 定义分类器部分\n",
    "        self.classifier = Sequential(\n",
    "            Flatten(),\n",
    "            Linear(512 * 7 * 7, 4096),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.5),\n",
    "            Linear(4096, 4096),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.5),\n",
    "            Linear(4096, 10)  # 假设是 10 分类任务\n",
    "        )\n",
    "        # 对分类器部分的全连接层进行 Xavier 初始化\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, Linear):\n",
    "                if layer is not self.classifier[-1]:\n",
    "                    kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "                else:\n",
    "                    xavier_uniform_(layer.weight)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "# 定义损失函数和优化器\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "import time\n",
    "\n",
    "\n",
    "def train_model(train_loader, model):\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # 训练的轮数\n",
    "    num_epochs = 10\n",
    "    # 初始化 running_loss\n",
    "    running_loss = 0.0\n",
    "    # 记录训练开始时间\n",
    "    start_time = time.time()\n",
    "    # 遍历epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # 遍历训练数据\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # 将数据移动到GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 计算模型输出\n",
    "            y_hat = model(inputs)\n",
    "            # 计算损失\n",
    "            loss = criterion(y_hat, labels)\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        # 打印每个 epoch 的损失\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}')\n",
    "        # 每个 epoch 结束后重置 running_loss\n",
    "        running_loss = 0.0\n",
    "    # 记录训练结束时间\n",
    "    end_time = time.time()\n",
    "    # 计算训练总时间\n",
    "    total_time = end_time - start_time\n",
    "    # 转换为分钟和秒\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = total_time % 60\n",
    "    print(\n",
    "        f'Training completed in {minutes} minutes and {seconds:.2f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "\n",
    "\n",
    "def evaluate_model(test_loader, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        # 将数据移动到GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 计算模型输出\n",
    "        y_hat = model(inputs)\n",
    "        # 转换为 numpy 数据类型\n",
    "        y_hat = y_hat.detach().cpu().numpy()\n",
    "        actual = labels.cpu().numpy()\n",
    "        # 转换为类标签\n",
    "        y_hat = argmax(y_hat, axis=1)\n",
    "        # 为stack格式化\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        y_hat = y_hat.reshape((len(y_hat), 1))\n",
    "        # 存储\n",
    "        predictions.append(y_hat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # 计算准确率\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.3040281296386134\n",
      "Epoch 2/10, Loss: 2.303758852317205\n",
      "Epoch 3/10, Loss: 2.3036516770682374\n",
      "Epoch 4/10, Loss: 2.303596309078929\n",
      "Epoch 5/10, Loss: 2.3034174064236224\n",
      "Epoch 6/10, Loss: 2.3033584716070035\n",
      "Epoch 7/10, Loss: 2.303367879384619\n",
      "Epoch 8/10, Loss: 2.3034772220474986\n",
      "Epoch 9/10, Loss: 2.3036128797799424\n",
      "Epoch 10/10, Loss: 2.303322779248133\n",
      "Training completed in 41 minutes and 23.63 seconds.\n",
      "Overall Accuracy: 0.101\n"
     ]
    }
   ],
   "source": [
    "# 产生实例,并且将实例放入GPU\n",
    "VggNet_model = VggNet().to(device)\n",
    "# 训练实例\n",
    "train_model(train_loader, VggNet_model)\n",
    "# 评估实例整体准确率\n",
    "acc = evaluate_model(test_loader, VggNet_model)\n",
    "print('Overall Accuracy: %.3f' % acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
